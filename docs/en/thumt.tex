\documentclass{article}
\usepackage{everb}
\usepackage{verbatim}
\usepackage{natbib}
\usepackage[colorlinks, linkcolor=red, anchorcolor=purple,citecolor=blue]{hyperref}

% \cite{Liu:05} => Liu et al. (2005)
% \citep{Liu:05} => (Liu et al., 2005)

\everbsetup{bgcolor={[rgb]{0.92,0.92,1.00}}}

\def\argmax{\mathop{\rm argmax}}%
\def\argmin{\mathop{\rm argmin}}%
\def\max{\mathop{\rm max}}%

\title{THUMT: An Open-Source Toolkit for Neural Machine Translation}
\author{The Tsinghua Natural Language Processing Group}
\date{December, 2017}

\begin{document}
\maketitle

\section{Introduction}
Machine translation, which investigates the use of computer to translate human languages automatically, is an important task in natural language processing an artificial intelligence. With the availability of bilingual, machine-readable texts, data-driven approaches to machine translation have gained wide popularity since 1990s. Recent several years have witnessed the rapid development of end-to- end neural machine translation (NMT)~\citep{Sutskever:14,Bahdanau:15}. Capable of learning representations from data, NMT has quickly replaced conventional statistical machine translation (SMT)~\citep{Brown:93,Koehn:03,Chiang:05} to become the new de facto method in practical MT systems~\citep{Wu:16}.

On top of \href{http://tensorflow.org)}{TensorFlow}, THUMT is an open-source toolkit for neural machine translation developed by the Tsinghua Natural Language Processing Group. THUMT has the following features:
\begin{enumerate}
\item \emph{Attention-based translation model}. THUMT implements the standard attention-based encoder-decoder framework for NMT\citep{Bahdanau:15} as well as the latest Transformer architecture~\citep{vaswani2017attention}.
\item \emph{Minimum risk training}. Besides standard maximum likelihood estimation (MLE), THUMT also supports minimum risk training (MRT)~\citep{Shen:16} that aims to find a set of model parameters that minimize the expected loss calculated using evaluation metrics such as BLEU~\citep{Papineni:02} on the training data.
\end{enumerate}


\section{Installation}

\subsection{System Requirements}
THUMT supports Linux i686 and Mac OSX. The following third-party software is required to install THUMT:
\begin{enumerate}
\item Python version 2.7.0 or higher.
\item JRE 1.6 or higher (optional, only used for visualization).
\end{enumerate}

\subsection{Installing Prerequisites}
We recommend using pip to install the prerequisites of THUMT. The installation starts with python-pip:
\begin{everbatim}
apt-get install python-pip
\end{everbatim}

Then, run the following two commands to install argparse and TensorFlow (version $\ge$ 1.4.0):

\begin{everbatim}
pip install argparse
pip install tensorflow-gpu
\end{everbatim}

\subsection{Installing THUMT}
The source code of THUMT is available both at \href{http://thumt.thunlp.org}{the toolkit website} (stable release) and GitHub (latest version).

Here is a brief guide on how to install THUMT.

\subsubsection{Step1: Unpacking}
Unpack the package using the following command:

\begin{everbatim}
tar xvfz THUMT.tar.gz
\end{everbatim}

Entering the \verb|THUMT| folder, you may find two folders (\verb|thumt|, \verb|data|) and three files (\verb|LICENSE|, \verb|UserManual.en.pdf| and \verb|UserManual.zh.pdf|):
\begin{enumerate}
\item \verb|thumt|: the source code.
\item \verb|data|: toy training, validation, and test datasets.
\item \verb|docs|: source of the documents.
\item \verb|LICENSE|: license statement.
\item \verb|UserManual.en.pdf|: this document.
\item \verb|UserManual.zh.pdf|: the Chinese version of this document.
\end{enumerate}


\subsubsection{Step 2: Modifying Environment Variables}
We highly recommend running THUMT on GPU servers. Suppose THUMT runs on NVIDIA GPUs with \href{https://developer.nvidia.com/cuda-toolkit}{the CUDA toolkit} version 8.0 installed. Users need to set environment variables to enable the GPU support:
\begin{everbatim}
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export PYTHONPATH=/PATH/TO/THUMT:$PYTHONPATH
\end{everbatim}

To set these environment variable permanently for all future bash sessions, users can simply add the above two lines to the \verb|.bashrc| file in your \verb|$HOME| directory, where \verb|/PATH/TO/THUMT| is the path of \verb|THUMT|.

\section{User Guide}

\subsection{Data Preparation}
Running THUMT involves three types of datasets:

\begin{enumerate}
\item {\em Training set}: a set of parallel sentences used for training NMT models.。
\item {\em Validation set}: a set of source sentences paired with single or multiple target translations used for model selection and hyper-parameter optimization.
\item {\em Test set}: a set of source sentences paired with single or multiple target translations used for evaluating translation performance on unseen texts.
\end{enumerate}

\subsubsection{Training Set}
A training set is used for training NMT models. It often consists of two files: one file for source sentences and the other for corresponding target sentences. In the \verb|data| folder, there is an example source file \verb|train.src| of a toy training set that contains seven sentences\footnote{The Chinese text is romanized for readability.}:

\begin{everbatim}
wo hen xihuan yinyue .
wo bu xihuan huahua .
ni xihuan yinyue me ?
shide , wo ye xihuan yiyue .
ta ye xihuan yinyue .
ta yidian dou bu xihuan yiyue .
ta hen xinhuan huahua .
\end{everbatim}

The corresponding target file \verb|train.trg| is shown below:
\begin{everbatim}
i like music very much .
i do not like painting .
do you like music ?
yes , i like music too .
he also likes music .
she does not like music at all .
she likes painting very much .
\end{everbatim}

Note that each line in the source and target files only contains one tokenized sentence. The source and target sentences with the same line number are translationally equivalent.

Our toy training set only contains seven sentence pairs. In practice, NMT often requires millions of sentence pairs to achieve reasonable translation performance.

\subsubsection{Validation Set}

A validation set is used for model selection and hyper-parameter optimization. During training, THUMT evaluates intermediate models on the validation set periodically. The model that obtains the highest BLEU score on the validation set is chosen as the final learned model.
A validation set consists of one source file and one or more target files. There is an example source file \verb|valid.src| of a toy validation set in the \verb|data| folder:

\begin{everbatim}
wo hen xihuan huahua .
wo bu xihuan yinyue .
\end{everbatim}

In our toy validation set, there is only one target file \verb|valid.trg| that contains the reference translations of \verb|valid.src|:
\begin{everbatim}
i like painting very much .
i do not like music .
\end{everbatim}

Due to the diversity of natural languages, one source sentence often corresponds to multiple reference translations. THUMT also supports multiple references.

\subsubsection{Test set}
A test set is used for evaluating the learned NMT model on unseen source text. Like a validation set, a test set also contains one source file and one or more target files. It shares the same naming scheme for multiple references with the validation set.

In the \verb|data| folder, there is one source file \verb|test.src|:
\begin{everbatim}
ta xihuan huahua me ?
ta yidian dou bu xihuan huahua .
\end{everbatim}

The corresponding target file \verb|test.trg| is shown below:
\begin{everbatim}
does she like painting ?
he does not like painting at all .
\end{everbatim}

In our toy data, validation and test sets contain only two sentences. In practice, thousands of sentences are needed for both validation and test sets.

\subsection{Training}

\subsection{Data preprocessing}
The most common approach to achieve open vocabularies is to use the Byte Pair Encoding (BPE). The codes of BPE can be found at \href{https://github.com/rsennrich/subword-nmt}{Subword-NMT}.

To encode the training corpora using BPE, you need to generate BPE operations first. The following command will create a file named \verb|bpe32k|, which contains 32k BPE operations. It also outputs two dictionaries named \verb|vocab.src| and \verb|vocab.trg|.
\begin{everbatim}
python subword-nmt/learn_joint_bpe_and_vocab.py
  --input train.src train.trg -s 32000 -o bpe32k
  --write-vocabulary vocab.src vocab.trg
\end{everbatim}

You also need to encode the training corpora, validation set and test set using the generated BPE operations and dictionaries.
\begin{everbatim}
  python subword-nmt/apply_bpe.py
    --vocabulary vocab.src
    --vocabulary-threshold 50
    -c bpe32k < train.src > train.32k.src
  python subword-nmt/apply_bpe.py
    --vocabulary vocab.trg
    --vocabulary-threshold 50
    -c bpe32k < train.trg > train.32k.trg
  python subword-nmt/apply_bpe.py
    --vocabulary vocab.src
    --vocabulary-threshold 50
    -c bpe32k < valid.src > valid.32k.src
  python subword-nmt/apply_bpe.py
    --vocabulary vocab.trg --vocabulary-threshold 50
    -c bpe32k < valid.trg > valid.32k.trg
  python subword-nmt/apply_bpe.py
    --vocabulary vocab.src
    --vocabulary-threshold 50
    -c bpe32k < test.src > test.32k.src
\end{everbatim}
The original segmentation can be restored with the following command:
\begin{everbatim}
sed -r 's/(@@ )|(@@ ?$)//g' < input > output
\end{everbatim}


\subsection{Shuffling Corpora}
It is helpful to shuffle training corpora first and we provide \verb|shuffle_corpus.py| script to achieve this functionality, which has the following options：
\begin{everbatim}
usage: shuffle_corpus.py [-h] --corpus CORPUS [CORPUS ...]
                         [--suffix SUFFIX]
                         [--seed SEED]

Shuffle corpus

optional arguments:
  -h, --help            show this help message and exit
  --corpus CORPUS [CORPUS ...]
                        input corpora
  --suffix SUFFIX       Suffix of output files
  --seed SEED           Random seed
\end{everbatim}

The meaning of arguments is shown below:
\begin{enumerate}
\item \verb|corpus|: the input corpus.
\item \verb|--suffix|：the suffix of output filename.
\item \verb|--seed|: the random seed.
\item \verb|--help|：show the help message.
\end{enumerate}


\subsubsection{Generating Vocabularies}
We have to build vocabularies before training an NMT model. The \verb|build_vocab.py| in the \verb|scripts| folder can be used to create vocabularies. It has the following options:
\begin{everbatim}
usage: build_vocab.py [-h] [--limit LIMIT] [--control CONTROL]
                      corpus output

Create vocabulary

positional arguments:
  corpus             input corpus
  output             Output vocabulary name

optional arguments:
  -h, --help         show this help message and exit
  --limit LIMIT      Vocabulary size
  --control CONTROL  Add control symbols to vocabulary.
                     Control symbols are separated by comma.
\end{everbatim}

The meaning of arguments is shown below:
\begin{enumerate}
\item \verb|corpus|: path to source or target corpus.
\item \verb|output|: the name of output vocabulary.
\item \verb|--limit|: control the size of vocaublary.
\item \verb|--control|: a comma separated string represents the control symbols. For example, the string ``\verb|<pad>,<eos>,<unk>|'' will add three control symbols to the beginning of the vocabulary.
\item \verb|--help|: show the help message.
\end{enumerate}

\subsubsection{The Python Script for Training}
THUMT implements the traditional sequence to sequence model \textsc{Seq2Seq}~\citep{Sutskever:14}, standard attention-based encoder-decoder framework for neural machine translation \textsc{RNNsearch}~\citep{Bahdanau:15} as well as the attention-based Transformer architecture~\citep{vaswani2017attention}.

The \verb|trainer.py| script in the \verb|bin| folder is used for training NMT models. Its arguments are listed as follows:
\begin{everbatim}
usage: trainer.py [<args>] [-h | --help]

Training neural machine translation models

optional arguments:
  -h, --help            show this help message and exit
  --input INPUT INPUT   Path of source and target corpus
  --output OUTPUT       Path to saved models
  --vocabulary VOCABULARY VOCABULARY
                        Path of source and target vocabulary
  --validation VALIDATION
                        Path of validation file
  --references REFERENCES [REFERENCES ...]
                        Path of reference files
  --model MODEL         Name of the model
  --parameters PARAMETERS
                        Additional hyper parameters
\end{everbatim}

We distinguish between \textit{required} and \textit{optional} arguments. Users must specify the following required arguments to run the \verb|trainer.py| script:
\begin{enumerate}
\item \verb|--model|: the name of model architecture. It can be \verb|seq2seq|, \verb|rnnsearch| or \verb|transformer|。
\item \verb|--input|: the path of source and target corpora.
\item \verb|--vocabulary|: the path of source and target vocabularies.
\end{enumerate}

The optional arguments of the \verb|trainer.py| script can be omitted in a com- mand. If an optional argument has a default value, the default value will be used in training if the argument is omitted in the command-line argument list. These optional arguments are listed as follows:
\begin{enumerate}
\item \verb|--output|: the path to save checkpoints. The default value is \verb|train| directory in the current path.
\item \verb|--validation|: the path to the validation set. The best model will saved in \verb|train/eval| directory.
\item \verb|--references|: the path to references. Multiple references are also supported.
\item \verb|--parameters|: other optional parameters which will be described in the next subsection. The parameters is a string containing comma-separated \verb|name=value| pairs. Please refer to the document of \verb|tf.contrib.training.parse_values|.
\item \verb|--help|：show this help message.
\end{enumerate}


\subsection{General Parameters}
\begin{enumerate}
\item \verb|num_threads|: the number of threads used in data processing.
\item \verb|buffer_size|: the buffer size used in data processing.
\item \verb|batch_size|: the batch size used in training stage.
\item \verb|constant_batch_size|: if set to true, then each batch will contains \verb|batch_size| sentences. Otherwise, each batch will contains approximately \verb|batch_size| tokens.
\item \verb|max_length|: the maximum length of training sentences.
\item \verb|train_steps|: the total number of steps in the training stage.
\item \verb|update_cycle|: update the parameters every $N$ iterations. If you only have 1GPU and want to achieve the same results trained under $N$ GPUs, you can set \verb|update_cycle=N|.
\item \verb|save_checkpoint_steps|: save the checkpoint every \verb|N| steps.
\item \verb|save_checkpoint_secs|: save the checkpoint every \verb|N| second.
\item \verb|initializer|: choose different initializing functions. The possible values are \verb|uniform_unit_scaling|,\\ \verb|uniform|，\verb|normal| and \verb|normal_unit_scaling|.
\item \verb|initializer_gain|: set the parameter of the initializer.
\item \verb|learning_rate|: adjust the learning rate.
\item \verb|learning_rate_decay|: set different learning rate decay function. The possible values are \verb|noam|(used in the transformer architecture)、\verb|piecewise_constant|(see \verb|tf.train.piecewise_constant|) and \verb|none|.
\item \verb|learning_rate_boundaries|: see \verb|tf.train.piecewise_constant| for more references.
\item \verb|learning_rate_values|: see \verb|tf.trian.piecewise_constant| for more references.
\item \verb|keep_checkpoint_max|: the maximum number of checkpoints to keep.
\item \verb|keep_top_checkpoint_max|: the maximum number of top performed checkpoints to keep.
\item \verb|eval_steps|: validate the model every \verb|N| steps.
\item \verb|eval_secs|: validate the model every \verb|N| seconds.
\item \verb|eval_batch_size|: the batch size used when validating.
\item \verb|decode_alpha|: the length penalty term in the beam search. See \citep{Wu:16} for more references.
\item \verb|beam_size|: the beam size of beam search.
\item \verb|decode_length|: the maximum length of decoded length, which is the length of source sentence plus this value.
\item \verb|device_list|: the device id of GPUs. For example, set \verb|device_list=[0,1]| will use GPU 0 and 1. Each GPU will process \verb|batch_size| data.
\end{enumerate}

\subsection{Parameters specific to Seq2Seq}
\begin{enumerate}
\item \verb|rnn_cell|: the recurrent unit. Currently, only LSTM is supported.
\item \verb|embedding_size|: the size of source and target embedding.
\item \verb|hidden_size|: the hidden units of RNN layer.
\item \verb|num_hidden_layers|: the number of RNN layers.
\item \verb|dropout|: dropout rate used in the network.
\item \verb|label_smoothing|: the value of label smoothing.
\item \verb|reverse_source|: if set to true, reverse the source sentence automatically~\citep{Sutskever:14}.
\item \verb|use_residual|: whether to use residual connections when multi-layered LSTM is employed.
\end{enumerate}
\subsection{Parameters specific to RNNsearch}
\begin{enumerate}
\item \verb|rnn_cell|: the recurrent unit. Currently, only GRU is supported.
\item \verb|embedding_size|: the size of source and target embedding.
\item \verb|hidden_size|: the hidden units of RNN layer.
\item \verb|maxnum|: the hidden units of maxout layer.
\item \verb|dropout|: dropout rate used in the network.
\item \verb|label_smoothing|: the value of label smoothing.
\end{enumerate}
\subsection{Parameters specific to Transformer}
\begin{enumerate}
\item \verb|hidden_size|: the embedding size and hidden size of the network.
\item \verb|filter_size|: the hidden size of Feed-forward layer.
\item \verb|num_encoder_layers|: the number of encoder layers.
\item \verb|num_decoder_layers|: the number of decoder layers.
\item \verb|num_heads|: the number of attention heads used in attention mechanism.
\item \verb|shared_embedding_and_softmax_weights|: whether to share the embedding and softmax weights.
\item \verb|shared_source_target_embedding|: whether to share the source and target embedding.
\item \verb|residual_dropout|: the dropout rate used in residual connection.
\item \verb|attention_dropout|: the dropout rate used in attention mechanism.
\item\verb|relu_dropout|: the dropout rate used in feed forward layer.
\item \verb|label_smoothing|: the value of label smoothing.
\end{enumerate}


\subsection{Test}
\subsubsection{The Python Script for Test}
The \verb|translator.py| in the \verb|bin| folder is used to translate unseen test sentences. It has the following arguments:

\begin{everbatim}
usage: translator.py [<args>] [-h | --help]

Translate using existing NMT models

optional arguments:
  -h, --help            show this help message and exit
  --input INPUT         Path of input file
  --output OUTPUT       Path of output file
  --checkpoints CHECKPOINTS [CHECKPOINTS ...]
                        Path of trained models
  --vocabulary VOCABULARY VOCABULARY
                        Path of source and target vocabulary
  --models MODELS [MODELS ...]
                        Name of the model
  --parameters PARAMETERS
                        Additional hyper parameters
\end{everbatim}

Users must specify the following required arguments to run the \verb|translator.py| script:

\begin{enumerate}
\item \verb|--input|: the path to input file.
\item \verb|--output|: the path to translated results.
\item \verb|--checkpoints|：the path to trained checkpoints.
\item \verb|--vocabulary|: the path to source and target vocabulary.
\item \verb|--models|: the name of used architecture.
\end{enumerate}

The optional argument of \verb|translator.py| is listed below:
\begin{enumerate}
\item \verb|--parameters|: Change parameters used in decoding. Such as \verb|beam_size| or the GPU id.
\item \verb|--help|: Show the help message.
\end{enumerate}

\subsubsection{Decoding}
The source file of the test set \verb|test.src| in the data folder is
\begin{everbatim}
ta xihuan huahua me ?
ta yidian dou bu xihuan huahua .
\end{everbatim}

Given a directory to trained models \verb|train|, please run the following command to translate the test set without evaluation:

\begin{everbatim}
python /PATH/TO/THUMT/thumt/bin/translator.py
  --models transformer
  --input test.src
  --output test.trans
  --checkpoints train
  --vocabulary vocab.zh vocab.en
  --parameters=device_list=[0]
\end{everbatim}
Note that \verb|test.trans| is the output of THUMT.

\subsection{Model Averaging}
Model averaging is also called checkpoint ensemble, which is used to average the checkpoints saved during training. The \verb|checkpint_averaging.py| script is used to average checkpoints, which has the following arguments:
\begin{everbatim}
usage: average.py [<args>] [-h | --help]

Average checkpoints

optional arguments:
  -h, --help            show this help message and exit
  --path PATH           checkpoint dir
  --checkpoints CHECKPOINTS
                        number of checkpoints to use
  --output OUTPUT       output path
\end{everbatim}

The meaning of arguments is shown below:
\begin{enumerate}
\item \verb|path|: the path to checkpoints, which is the \verb|output| argument specified during training.
\item \verb|--checkpoints|: the number of checkpoints to average. It will load \verb|N| latest checkpoints.
\item \verb|--output|: the output directory.
\item \verb|--help|：show the help message.
\end{enumerate}


\subsection{Examples}
The following is the command we used to train a base Transformer model. This model shares the source, target and softmax weights. We employed 4 GPUs to accelerate training:
\begin{everbatim}
python /PATH/TO/THUMT/thumt/bin/trainer.py
  --input train.tok.clean.bpe.32000.en.shuf
          train.tok.clean.bpe.32000.de.shuf
  --model transformer --output train/
  --vocabulary vocab.bpe.32000 vocab.bpe.32000
  --validation newstest2013.tok.bpe.32000.en
  --references newstest2013.tok.bpe.32000.de
  --parameters=batch_size=6250,device_list=[0,1,2,3],
               eval_steps=5000,train_steps=100000,
               save_checkpoint_steps=1500,
               shared_embedding_and_softmax_weights=true,
               shared_source_target_embedding=true
\end{everbatim}

Once the training stage is finished, we use th following command to average checkpoints:
\begin{everbatim}
python /PATH/TO/THUMT/thumt/scripts/checkpoint_averaging.py
  --path train --checkpoints 5 --output average
\end{everbatim}

Finally, we use the \verb|translator.py| to translate the test set.
\begin{everbatim}
python /PATH/TO/THUMT/thumt/bin/translator.py
  --model transformer
  --checkpoints average
  --input newstest2014.tok.bpe.32000.en
  --vocabulary vocab.bpe.32000 vocab.bpe.32000
  --output output.txt
\end{everbatim}

\bibliographystyle{apalike}
\bibliography{thumt}

\end{document}
